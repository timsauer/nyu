{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "* Today we will show how to train a ConvNet using PyTorch\n",
    "* We will also illustrate how the ConvNet makes use of specific assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To perform well, we need to incorporate some prior knowledge about the problem\n",
    "\n",
    "* Assumptions helps us when they are true\n",
    "* They hurt us when they are not\n",
    "* We want to make just the right amount of assumptions, not more than that\n",
    "\n",
    "## In Deep Learning\n",
    "\n",
    "* Many layers: compositionality\n",
    "* Convolutions: locality + stationarity of images\n",
    "* Pooling: Invariance of object class to translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset (MNIST)\n",
    "\n",
    "\n",
    "We can use some PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cfc1f1dc6d4a2396fbdd8e801ea9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef5b9a3e2ad4fdda88abe11fab07a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bb77c27a41477ba2c38bec8173efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788e6555c30b4901b6ef26250770231e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothysauer/opt/miniconda3/envs/pDL/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1603740477510/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i+10)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on a GPU: device string\n",
    "\n",
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a small fully-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6442\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.306099\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.963330\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.365482\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 1.030642\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.844536\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.555971\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.699803\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.550561\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.576361\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.372242\n",
      "\n",
      "Test set: Average loss: 0.5408, Accuracy: 8448/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ConvNet with the same number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6422\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.291160\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.716820\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.502679\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.204932\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.440939\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.139605\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.139909\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.113762\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.216999\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.302247\n",
      "\n",
      "Test set: Average loss: 0.1367, Accuracy: 9594/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "* Use of convolution: Locality and stationarity in images\n",
    "* Pooling: builds in some translation invariance\n",
    "\n",
    "# What happens if the assumptions are no longer true?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28*28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet with permuted pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected with Permuted Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8    # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
    "\n",
    "* ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "* It loses performance when this assumption is wrong\n",
    "* The fully-connected network does not make this assumption\n",
    "* It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "* But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(('NN image', 'CNN image',\n",
    "         'CNN scrambled', 'NN scrambled'),\n",
    "        accuracy_list, width=0.4)\n",
    "plt.ylim((min(accuracy_list)-5, 96))\n",
    "plt.ylabel('Accuracy [%]')\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(20)\n",
    "plt.title('Performance comparison');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model_fnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
